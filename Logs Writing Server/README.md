# Logs Writing Server

This folder contains the **Logs Writing Server** microservice, responsible for logging company and recruiter actions asynchronously by consuming messages from a Kafka message broker. This service enables reliable, scalable, and decoupled logging of important actions, allowing companies to review activity history as needed.

## Overview

The Logs Writing Server is a Node.js service that listens to a Kafka topic for log events generated by other services in the system. When a message is received, it parses the event and writes the log entry to a PostgreSQL database. This design ensures that logging is performed asynchronously and does not block user-facing operations.

## Features

- **Asynchronous Logging:** Consumes log events from Kafka, decoupling logging from main application logic.
- **Idempotent Writes:** Ensures each log entry is written exactly once using PostgreSQL's `ON CONFLICT DO NOTHING`.
- **Supports Deletion:** Can handle both insertion and deletion of logs based on message type. This is especially useful for maintaining consistency: if an error occurs on the producer side after publishing a log message to Kafka (for example, if a database transaction fails to commit), a corresponding delete message can be sent to remove the previously written log. This ensures that the logs accurately reflect only successful actions and helps prevent misleading log entries.
- **Retry Mechanism:** Uses exponential backoff to retry failed database operations.
- **Scalable:** Can be scaled horizontally by running multiple instances.
- **Error Handling:** Logs errors and continues processing without crashing.

## Architecture

1. **Kafka Consumer:** Listens to the `logs` topic for new log events.
2. **Database Access:** Uses a PostgreSQL connection pool for efficient writes.
3. **Retry Logic:** Uses the `async-retry` library to retry failed database operations.
4. **Offset Management:** Commits Kafka offsets only after successful log writing, ensuring at-least-once delivery.

## Folder Structure

```
Logs Writing Server/
├── config/
│   ├── config.js         # Service configuration and environment variables
│   ├── db.js             # PostgreSQL connection pool
│   └── kafka.js          # Kafka consumer setup
├── src/
│   └── app.js            # Main service entry point and Kafka consumer logic
├── .dockerignore
├── docker-compose.yml
├── Dockerfile
├── package.json
└── .env                  # Environment variables
```

## Configuration

All configuration is managed via environment variables, loaded in [`config.js`](/Logs%20Writing%20Server/config/config.js). This includes database credentials and Kafka broker addresses.

## Environment Variables

The following environment variables are required (see [`config.js`](/Logs%20Writing%20Server/config/config.js)):

- `MASTER_NAME` - PostgreSQL host
- `DB_USER` - PostgreSQL user
- `DB_USER_PASSWORD` - PostgreSQL password
- `DB_PORT` - PostgreSQL port
- `DB_NAME` - PostgreSQL database name
- `KAFKA_BROKER1`, `KAFKA_BROKER2` - Kafka broker addresses

## Kafka Topic

The service listens to the topic specified by the `logs_topic` variable. Other services publish log events to this topic.

## How It Works

1. **Startup:** The service connects to Kafka and subscribes to the logs topic.
2. **Message Handling:** For each message:
   - Parses the event type and relevant fields.
   - If `type` is truthy, inserts a new log entry (idempotently).
   - If `type` is falsy, deletes the log entry with the given ID.
   - Uses a retry mechanism for database operations.
   - Commits the Kafka offset only after a successful operation.
3. **Error Handling:** If an operation fails after all retries, the error is logged, and the message will be retried on the next run.

See [`app.js`](/Logs%20Writing%20Server/src/app.js) for the main logic.

## Asynchronous Retries

If writing to the database fails (for example, due to a temporary connection issue), the service uses the [`async-retry`](https://www.npmjs.com/package/async-retry) library to retry the operation up to 4 times with exponential backoff. The Kafka offset is only committed after a successful write, ensuring that failed messages are retried automatically and no logs are lost.

## Running the Service

### Prerequisites

- Access to a running Kafka cluster
- Access to a PostgreSQL database

### Build and run using Docker Compose:

```sh
docker compose up -d
```
